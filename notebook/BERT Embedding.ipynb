{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import warnings\n",
    "import string\n",
    "import joblib\n",
    "import multiprocessing\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import *\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total tweets: 1297358\n",
      "There are 282228 tweets we get.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Happy New Year, everyone! Like atUser just tol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>trading forex binaryoptions China steps up scr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Forex Forum -   url replies to: Best Forex Sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Forex Forum -  goosebone replies to: Let is Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Learning MQL4: must-do or just for fun? via /r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                        clean_tweet\n",
       "0  2017-01-01  Happy New Year, everyone! Like atUser just tol...\n",
       "5  2017-01-01  trading forex binaryoptions China steps up scr...\n",
       "6  2017-01-01  Forex Forum -   url replies to: Best Forex Sig...\n",
       "7  2017-01-01  Forex Forum -  goosebone replies to: Let is Tr...\n",
       "8  2017-01-01  Learning MQL4: must-do or just for fun? via /r..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tweets(tweets_file=\"../data/preprocessed_tweet_20201619.csv\", \n",
    "                from_date=\"2017-01-01\", \n",
    "                to_date=\"2020-06-01\", \n",
    "                count=10):\n",
    "    cols = [\"date\", \"time\", \"username\", \"tweet\", \"clean_tweet\", \"hashtags\", \n",
    "            \"likes_count\", \"replies_count\", \"retweets_count\", \"slang_count\"]\n",
    "    df = pd.read_csv(tweets_file, usecols=cols)\n",
    "    print(\"# of total tweets: {}\".format(df.shape[0]))\n",
    "    df.sort_values(by=\"date\", ascending=True, inplace=True)\n",
    "    df.set_index('date', inplace=True)\n",
    "    df = df.loc[from_date:to_date]\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df = df[df.clean_tweet.str.count('\\s+').gt(count)]\n",
    "    print(\"There are {} tweets we get.\".format(df.shape[0]))\n",
    "    return df\n",
    "\n",
    "df = load_tweets(from_date=\"2017-01-01\", to_date=\"2020-06-17\")\n",
    "df_simplied = df[[\"date\", \"clean_tweet\"]]\n",
    "df_simplied.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertVectorizer():\n",
    "    \"\"\"\n",
    "    BERT Vectorizer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    def vectorize(self, text):\n",
    "        tokenized_text = self.tokenizer.encode_plus(\n",
    "            text=text, \n",
    "            max_length=32, \n",
    "            add_special_tokens=True, \n",
    "            return_token_type_ids=False, \n",
    "            pad_to_max_length=True, \n",
    "            return_attention_mask=True, \n",
    "            return_tensors=\"pt\")\n",
    "        with torch.no_grad(): \n",
    "            logits = self.model(\n",
    "                tokenized_text[\"input_ids\"].to(self.device), \n",
    "                tokenized_text[\"attention_mask\"].to(self.device))\n",
    "        vector = logits[1].to(\"cpu\").numpy()\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = BertVectorizer()\n",
    "df_simplied['tweet_vector'] = df_simplied['clean_tweet'].apply(lambda x: vectorizer.vectorize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tweets_vectors_df.gzip']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_simplied, \"../data/tweets_vectors_df.gzip\", compress=3)\n",
    "tweets_vectors_df = joblib.load(\"../data/tweets_vectors_df.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.concat([df, tweets_vectors_df.tweet_vector], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
