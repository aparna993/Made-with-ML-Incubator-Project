{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Learning\n",
    "\n",
    "Recently, the NLP science community has begun to pay increasing attention to zero-shot and few-shot applications, such as in the paper from OpenAI introducing GPT-3. This [demo](https://joeddav.github.io/blog/2020/05/29/ZSL.html) shows how ðŸ¤— Transformers can be used for zero-shot topic classification, the task of predicting a topic that the model has not been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import warnings\n",
    "import string\n",
    "import joblib\n",
    "import multiprocessing\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(tweets_file=\"../data/preprocessed_tweet_20201619.csv\", \n",
    "                from_date=\"2017-01-01\", \n",
    "                to_date=\"2020-06-01\", \n",
    "                count=10):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        tweet_file: directory\n",
    "        from_date: str\n",
    "        to_date: str\n",
    "        count: int (remove the rows which sentence length are less than certain integer)\n",
    "    \"\"\"\n",
    "    cols = [\"date\", \"time\", \"username\", \"tweet\", \"clean_tweet\", \"hashtags\", \n",
    "            \"likes_count\", \"replies_count\", \"retweets_count\", \"slang_count\"]\n",
    "    df = pd.read_csv(tweets_file, usecols=cols)\n",
    "    print(\"# of total tweets: {}\".format(df.shape[0]))\n",
    "    df.sort_values(by=\"date\", ascending=True, inplace=True)\n",
    "    df.set_index('date', inplace=True)\n",
    "    df = df.loc[from_date:to_date]\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df = df[df.clean_tweet.str.count('\\s+').gt(count)]\n",
    "    print(\"There are {} tweets we get.\".format(df.shape[0]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Embedding Approach\n",
    "A common approach to zero shot learning in the computer vision setting is to use an existing featurizer to embed an image and any possible class names into their corresponding latent representations (e.g. Socher et al. 2013). In the text domain, we have the advantage that we can trivially use a single model to embed both the data and the class names into the same space, eliminating the need for the data-hungry alignment step. We therefore decided to run some experiments with Sentence-BERT, a recent technique which fine-tunes the pooled BERT sequence representations for increased semantic richness, as a method for obtaining sequence and label embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-BERT\n",
    "Here's an example code snippet showing how this can be done using Sentence-BERT as our embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBert():\n",
    "    \"\"\"\n",
    "    A common approach to zero shot learning using Sentence-BERT.\n",
    "    Reference from https://joeddav.github.io/blog/2020/05/29/ZSL.html\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('deepset/sentence_bert')\n",
    "        self.model = AutoModel.from_pretrained('deepset/sentence_bert')\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "    def get_similarity(self, sentence, labels):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sentence: str\n",
    "            label: list\n",
    "        \"\"\"\n",
    "        # Run inputs through model and mean-pool over the sequence dimension to get sequence-level representations\n",
    "        inputs = self.tokenizer.batch_encode_plus(\n",
    "            [sentence] + labels,\n",
    "            return_tensors='pt',\n",
    "            pad_to_max_length=True)\n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        attention_mask = inputs['attention_mask'].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_ids, attention_mask=attention_mask)[0]\n",
    "        sentence_rep = output[:1].mean(dim=1)\n",
    "        label_reps = output[1:].mean(dim=1)\n",
    "    \n",
    "        # Now find the labels with the highest cosine similarities to the sentence\n",
    "        similarities = F.cosine_similarity(sentence_rep, label_reps)\n",
    "        closest = similarities.argsort(descending=True)\n",
    "        \n",
    "        sim_dict = defaultdict()\n",
    "        for ind in closest:\n",
    "            sim_dict[labels[ind]] = (similarities[ind].item())\n",
    "            \n",
    "        return sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total tweets: 1297358\n",
      "There are 282228 tweets we get.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10fe0db66164feaa73239fa7a17e2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=282228.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_tweets(from_date=\"2017-01-01\", to_date=\"2020-06-17\")\n",
    "df = df[[\"date\", \"clean_tweet\"]]\n",
    "\n",
    "SB = SentenceBert()\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    sim_dict = SB.get_similarity(row[\"clean_tweet\"], ['forex', 'finance', 'politics'])\n",
    "    df.loc[index, 'forex'] = sim_dict[\"forex\"]\n",
    "    df.loc[index, 'finance'] = sim_dict[\"finance\"]\n",
    "    df.loc[index, 'politics'] = sim_dict[\"politics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tweets_zero_shot_df.gzip']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df, \"../data/tweets_zero_shot_df.gzip\", compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference\n",
    "We will now explore an alternative method which not only embeds sequences and labels into the same latent space where their distance can be measured, but that can actually tell us something about the compatibility of two distinct sequences out of the box. As a quick review, natural language inference (NLI) considers two sentences: a \"premise\" and a \"hypothesis\". The task is to determine whether the hypothesis is true (entailment) or false (contradiction) given the premise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART\n",
    "BART is sequence-to-sequence model trained with denoising as pretraining objective. The approach, proposed by Yin et al. (2019), uses a pre-trained MNLI sequence-pair classifier as an out-of-the-box zero-shot text classifier that actually works pretty well. The idea is to take the sequence we're interested in labeling as the \"premise\" and to turn each candidate label into a \"hypothesis.\" If the NLI model predicts that the premise \"entails\" the hypothesis, we take the label to be true. Here is a [demo](https://huggingface.co/zero-shot/) built by hugginface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
