{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An advanced Twitter scraping & OSINT tool written in Python that doesn't use Twitter's API.\n",
    "import twint\n",
    "\n",
    "# Solve compatibility issues with notebooks and RunTime errors.\n",
    "import nest_asyncio\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"twint/\")\n",
    "nest_asyncio.apply()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python preprocessing library.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable             Type       Description\n",
    "# Username             (string) - Twitter user's username\n",
    "# User_id              (string) - Twitter user's user_id\n",
    "# Search               (string) - Search terms\n",
    "# Geo                  (string) - Geo coordinates (lat,lon,km/mi.)\n",
    "# Location             (bool)   - Set to True to attempt to grab a Twitter user's location (slow).\n",
    "# Near                 (string) - Near a certain City (Example: london)\n",
    "# Lang                 (string) - Compatible language codes: https://github.com/twintproject/twint/wiki/Langauge-codes\n",
    "# Output               (string) - Name of the output file.\n",
    "# Elasticsearch        (string) - Elasticsearch instance\n",
    "# Timedelta            (int)    - Time interval for every request (days)\n",
    "# Year                 (string) - Filter Tweets before the specified year.\n",
    "# Since                (string) - Filter Tweets sent since date (Example: 2017-12-27).\n",
    "# Until                (string) - Filter Tweets sent until date (Example: 2017-12-27).\n",
    "# Email                (bool)   - Set to True to show Tweets that _might_ contain emails.\n",
    "# Phone                (bool)   - Set to True to show Tweets that _might_ contain phone numbers.\n",
    "# Verified             (bool)   - Set to True to only show Tweets by _verified_ users\n",
    "# Store_csv            (bool)   - Set to True to write as a csv file.\n",
    "# Store_json           (bool)   - Set to True to write as a json file.\n",
    "# Custom               (dict)   - Custom csv/json formatting (see below).\n",
    "# Show_hashtags        (bool)   - Set to True to show hashtags in the terminal output.\n",
    "# Limit                (int)    - Number of Tweets to pull (Increments of 20).\n",
    "# Count                (bool)   - Count the total number of Tweets fetched.\n",
    "# Stats                (bool)   - Set to True to show Tweet stats in the terminal output.\n",
    "# Database             (string) - Store Tweets in a sqlite3 database. Set this to the DB. (Example: twitter.db)\n",
    "# To                   (string) - Display Tweets tweeted _to_ the specified user.\n",
    "# All                  (string) - Display all Tweets associated with the mentioned user.\n",
    "# Debug                (bool)   - Store information in debug logs.\n",
    "# Format               (string) - Custom terminal output formatting.\n",
    "# Essid                (string) - Elasticsearch session ID.\n",
    "# User_full            (bool)   - Set to True to display full user information. By default, only usernames are shown.\n",
    "# Profile_full         (bool)   - Set to True to use a slow, but effective method to enumerate a user's Timeline.\n",
    "# Store_object         (bool)   - Store tweets/user infos/usernames in JSON objects.\n",
    "# Store_pandas         (bool)   - Save Tweets in a DataFrame (Pandas) file.\n",
    "# Pandas_type          (string) - Specify HDF5 or Pickle (HDF5 as default).\n",
    "# Pandas               (bool)   - Enable Pandas integration.\n",
    "# Index_tweets         (string) - Custom Elasticsearch Index name for Tweets (default: twinttweets).\n",
    "# Index_follow         (string) - Custom Elasticsearch Index name for Follows (default: twintgraph).\n",
    "# Index_users          (string) - Custom Elasticsearch Index name for Users (default: twintuser).\n",
    "# Index_type           (string) - Custom Elasticsearch Document type (default: items).\n",
    "# Retries_count        (int)    - Number of retries of requests (default: 10).\n",
    "# Resume               (int)    - Resume from a specific tweet id (**currently broken, January 11, 2019**).\n",
    "# Images               (bool)   - Display only Tweets with images.\n",
    "# Videos               (bool)   - Display only Tweets with videos.\n",
    "# Media                (bool)   - Display Tweets with only images or videos.\n",
    "# Replies              (bool)   - Display replies to a subject.\n",
    "# Pandas_clean         (bool)   - Automatically clean Pandas dataframe at every scrape.\n",
    "# Lowercase            (bool)   - Automatically convert uppercases in lowercases.\n",
    "# Pandas_au            (bool)   - Automatically update the Pandas dataframe at every scrape.\n",
    "# Proxy_host           (string) - Proxy hostname or IP.\n",
    "# Proxy_port           (int)    - Proxy port.\n",
    "# Proxy_type           (string) - Proxy type.\n",
    "# Tor_control_port     (int) - Tor control port.\n",
    "# Tor_control_password (string) - Tor control password (not hashed).\n",
    "# Retweets             (bool)   - Display replies to a subject.\n",
    "# Hide_output          (bool)   - Hide output.\n",
    "# Get_replies          (bool)   - All replies to the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "# 63 Top Forex Twitter Accounts: \n",
    "# https://www.forexcrunch.com/60-top-forex-twitter-accounts/\n",
    "# https://towardsdatascience.com/analyzing-tweets-with-nlp-in-minutes-with-spark-optimus-and-twint-a0c96084995f\n",
    "def tweets_dateframe(search, output_file, year=\"2020\"):\n",
    "    # Configure\n",
    "    c = twint.Config()\n",
    "    c.Search = search\n",
    "    c.Year = year\n",
    "    c.Lang = \"en\"\n",
    "    c.Pandas = True\n",
    "    c.Store_csv = True\n",
    "    c.Format = \"Username: {username} |  Tweet: {tweet}\"\n",
    "    c.Output = output_file\n",
    "    c.Hide_output = True\n",
    "    # c.Limit = 10000\n",
    "    # c.User_full = True\n",
    "    # c.Since = since\n",
    "    # c.Until = until\n",
    "\n",
    "    # Run\n",
    "    with HiddenPrints():\n",
    "        print(twint.run.Search(c))\n",
    "    \n",
    "    return \"Done scraping tweets!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.run:Twint:Feed:noDataExpecting value: line 1 column 1 (char 0)\n",
      "sleeping for 1.0 secs\n",
      "CRITICAL:root:twint.run:Twint:Feed:noDataExpecting value: line 1 column 1 (char 0)\n",
      "sleeping for 8.0 secs\n",
      "CRITICAL:root:twint.run:Twint:Feed:noDataExpecting value: line 1 column 1 (char 0)\n",
      "sleeping for 27.0 secs\n",
      "CRITICAL:root:twint.run:Twint:Feed:noDataExpecting value: line 1 column 1 (char 0)\n",
      "sleeping for 64.0 secs\n",
      "CRITICAL:root:twint.run:Twint:Feed:noDataExpecting value: line 1 column 1 (char 0)\n",
      "sleeping for 125.0 secs\n",
      "CRITICAL:root:twint.run:Twint:Feed:noDataExpecting value: line 1 column 1 (char 0)\n",
      "sleeping for 216.0 secs\n"
     ]
    }
   ],
   "source": [
    "for year in tqdm([\"2017\", \"2018\", \"2019\", \"2020\"]):\n",
    "    tweets_dateframe(search=\"FXstreetNews\", output_file=\"forex.csv\", year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"forex.csv\", \n",
    "                 usecols=[\"date\", \"time\", \"username\", \"tweet\", \"hashtags\", \"likes_count\", \"replies_count\", \"retweets_count\"])\n",
    "print(\"# of tweets: {}\".format(df.shape[0]))\n",
    "df.sort_values(by=\"date\", ascending=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
